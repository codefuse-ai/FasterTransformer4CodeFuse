[encoder]
vocab_size = ${vocab_size}
d_model = ${d_model}
d_kv = ${d_kv}
d_ff = ${d_ff}
num_layers = ${num_encoder_layers}
num_decoder_layers = ${num_decoder_layers}
num_heads = ${num_heads}
relative_attention_max_distance = 128
dropout_rate = 0.1
layer_norm_epsilon = 1e-06
initializer_factor = 1.0
feed_forward_proj = gated-silu
use_cache = False
dense_act_fn = silu
is_gated_act = True
return_dict = True
output_hidden_states = False
output_attentions = False
torchscript = False
torch_dtype = bfloat16
use_bfloat16 = False
tf_legacy_loss = False
pruned_heads = {}
tie_word_embeddings = True
is_encoder_decoder = False
is_decoder = False
cross_attention_hidden_size = None
add_cross_attention = False
tie_encoder_decoder = False
max_length = 20
min_length = 0
do_sample = False
early_stopping = False
num_beams = 1
num_beam_groups = 1
diversity_penalty = 0.0
temperature = 1.0
top_k = 50
top_p = 1.0
typical_p = 1.0
repetition_penalty = 1.0
length_penalty = 1.0
no_repeat_ngram_size = 0
encoder_no_repeat_ngram_size = 0
bad_words_ids = None
num_return_sequences = 1
chunk_size_feed_forward = 0
output_scores = False
return_dict_in_generate = False
forced_bos_token_id = None
forced_eos_token_id = None
remove_invalid_values = False
exponential_decay_length_penalty = None
architectures = ['T5ForConditionalGeneration']
finetuning_task = None
id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
label2id = {'LABEL_0': 0, 'LABEL_1': 1}
tokenizer_class = None
prefix = None
bos_token_id = None
pad_token_id = 0
eos_token_id = 1
sep_token_id = None
decoder_start_token_id = 0
task_specific_params = None
problem_type = None
_name_or_path = ul2-huggingface/
transformers_version = 4.21.1
model_type = t5
n_positions = 512
output_past = True
weight_data_type = fp32
relative_attention_num_buckets_or_max_pos_seq_len = 32

[decoder]
vocab_size = ${vocab_size}
d_model = ${d_model}
d_kv = ${d_kv}
d_ff = ${d_ff}
num_layers = ${num_encoder_layers}
num_decoder_layers = ${num_decoder_layers}
num_heads = ${num_heads}
relative_attention_max_distance = 128
dropout_rate = 0.1
layer_norm_epsilon = 1e-06
initializer_factor = 1.0
feed_forward_proj = gated-silu
use_cache = True
dense_act_fn = silu
is_gated_act = True
return_dict = True
output_hidden_states = False
output_attentions = False
torchscript = False
torch_dtype = bfloat16
use_bfloat16 = False
tf_legacy_loss = False
pruned_heads = {}
tie_word_embeddings = True
is_encoder_decoder = False
is_decoder = True
cross_attention_hidden_size = None
add_cross_attention = False
tie_encoder_decoder = False
max_length = 20
min_length = 0
do_sample = False
early_stopping = False
num_beams = 1
num_beam_groups = 1
diversity_penalty = 0.0
temperature = 1.0
top_k = 50
top_p = 1.0
typical_p = 1.0
repetition_penalty = 1.0
length_penalty = 1.0
no_repeat_ngram_size = 0
encoder_no_repeat_ngram_size = 0
bad_words_ids = None
num_return_sequences = 1
chunk_size_feed_forward = 0
output_scores = False
return_dict_in_generate = False
forced_bos_token_id = None
forced_eos_token_id = None
remove_invalid_values = False
exponential_decay_length_penalty = None
architectures = ['T5ForConditionalGeneration']
finetuning_task = None
id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
label2id = {'LABEL_0': 0, 'LABEL_1': 1}
tokenizer_class = None
prefix = None
bos_token_id = None
pad_token_id = 0
eos_token_id = 1
sep_token_id = None
decoder_start_token_id = 0
task_specific_params = None
problem_type = None
_name_or_path = ul2-huggingface/
transformers_version = 4.21.1
model_type = t5
n_positions = 512
output_past = True
weight_data_type = fp32
relative_attention_num_buckets_or_max_pos_seq_len = 32

[structure]
t5_with_bias = false
use_gated_activation = 1
position_embedding_type = relative
